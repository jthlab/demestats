{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b95b1a6",
   "metadata": {},
   "source": [
    "I have the tskit object saved and ready to be loaded. Next chunk is the data generating code, just uncomment it if you want to generate data yourself, it takes approx 1 min to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import msprime as msp\n",
    "# import demes\n",
    "# import demesdraw\n",
    "# import numpy as np\n",
    "\n",
    "# # Create demography object\n",
    "# demo = msp.Demography()\n",
    "\n",
    "# # Add populations\n",
    "# demo.add_population(initial_size=4000, name=\"anc\")\n",
    "# demo.add_population(initial_size=500, name=\"P0\", growth_rate=-np.log(3000 / 500)/66)\n",
    "# demo.add_population(initial_size=500, name=\"P1\", growth_rate=-np.log(3000 / 500)/66)\n",
    "\n",
    "# # Set initial migration rate\n",
    "# demo.set_symmetric_migration_rate(populations=(\"P0\", \"P1\"), rate=0.0001)\n",
    "\n",
    "# # population growth at 500 generations\n",
    "# demo.add_population_parameters_change(\n",
    "#     time=66,\n",
    "#     initial_size=3000,  # Bottleneck: reduce to 1000 individuals\n",
    "#     population=\"P0\",\n",
    "#     growth_rate=0\n",
    "# )\n",
    "# demo.add_population_parameters_change(\n",
    "#     time=66,\n",
    "#     initial_size=3000,  # Bottleneck: reduce to 1000 individuals\n",
    "#     population=\"P1\",\n",
    "#     growth_rate=0\n",
    "# )\n",
    "\n",
    "# # Migration rate change changed to 0.001 AFTER 500 generation (going into the past)\n",
    "# demo.add_migration_rate_change(\n",
    "#     time=66,\n",
    "#     rate=0.0005,\n",
    "#     source=\"P0\",\n",
    "#     dest=\"P1\"\n",
    "# )\n",
    "# demo.add_migration_rate_change(\n",
    "#     time=66,\n",
    "#     rate=0.0005,\n",
    "#     source=\"P1\",\n",
    "#     dest=\"P0\"\n",
    "# )\n",
    "\n",
    "# # THEN add the older events (population split at 1000)\n",
    "# demo.add_population_split(time=5000, derived=[\"P0\", \"P1\"], ancestral=\"anc\")\n",
    "\n",
    "# # Visualize the demography\n",
    "# g = demo.to_demes()\n",
    "# demesdraw.tubes(g, log_time=True)\n",
    "\n",
    "# sample_size = 10\n",
    "# samples = {f\"P{i}\": sample_size for i in range(2)}\n",
    "# anc = msp.sim_ancestry(samples=samples, demography=demo, recombination_rate=3.94 * 1e-8, sequence_length=1e8, random_seed = 12)\n",
    "# ts = msp.sim_mutations(anc, rate=2.54 * 1e-8, random_seed = 12)from __future__ import annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb6cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tskit\n",
    "\n",
    "ts = tskit.load(\"two_population_with_bottlneck.trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01859ae3",
   "metadata": {},
   "source": [
    "next chunk is just loading all the functions I wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Mapping, Sequence, Set, Tuple\n",
    "\n",
    "import diffrax as dfx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import numpy as np\n",
    "from jax import lax, vmap\n",
    "from jax.scipy.special import xlogy\n",
    "from jaxtyping import Array, Float, Scalar, ScalarLike\n",
    "\n",
    "from demestats.coal_rate import PiecewiseConstant\n",
    "from demestats.iicr import IICRCurve\n",
    "\n",
    "Path = Tuple[Any, ...]\n",
    "Var = Path | Set[Path]\n",
    "Params = Mapping[Var, float]\n",
    "\n",
    "\n",
    "def _dict_to_vec(d: Params, keys: Sequence[Var]) -> jnp.ndarray:\n",
    "    return jnp.asarray([d[k] for k in keys], dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def _vec_to_dict_jax(v: jnp.ndarray, keys: Sequence[Var]) -> Dict[Var, jnp.ndarray]:\n",
    "    return {k: v[i] for i, k in enumerate(keys)}\n",
    "\n",
    "\n",
    "def _vec_to_dict(v: jnp.ndarray, keys: Sequence[Var]) -> Dict[Var, float]:\n",
    "    return {k: float(v[i]) for i, k in enumerate(keys)}\n",
    "\n",
    "\n",
    "def compile(ts, subkey, a=None, b=None):\n",
    "    # using a set to pull out all unique populations that the samples can possibly belong to\n",
    "    pop_cfg = {\n",
    "        ts.population(ts.node(n).population).metadata[\"name\"] for n in ts.samples()\n",
    "    }\n",
    "    pop_cfg = {pop_name: 0 for pop_name in pop_cfg}\n",
    "\n",
    "    if a is None and b is None:\n",
    "        samples = jax.random.choice(subkey, ts.num_samples, shape=(2,), replace=False)\n",
    "        a, b = samples[0].item(0), samples[1].item(0)\n",
    "\n",
    "    spans = []\n",
    "    curr_t = None\n",
    "    curr_L = 0.0\n",
    "    for tree in ts.trees():\n",
    "        L = tree.interval.right - tree.interval.left\n",
    "        t = tree.tmrca(a, b)\n",
    "        if curr_t is None or t != curr_t:\n",
    "            if curr_t is not None:\n",
    "                spans.append([curr_t, curr_L])\n",
    "            curr_t = t\n",
    "            curr_L = L\n",
    "        else:\n",
    "            curr_L += L\n",
    "    spans.append([curr_t, curr_L])\n",
    "    data = jnp.asarray(spans, dtype=jnp.float64)\n",
    "    pop_cfg[ts.population(ts.node(a).population).metadata[\"name\"]] += 1\n",
    "    pop_cfg[ts.population(ts.node(b).population).metadata[\"name\"]] += 1\n",
    "    return data, pop_cfg\n",
    "\n",
    "\n",
    "def get_tmrca_data(ts, key=jax.random.PRNGKey(2), num_samples=200, option=\"random\"):\n",
    "    data_list = []\n",
    "    cfg_list = []\n",
    "    key, subkey = jr.split(key)\n",
    "    if option == \"random\":\n",
    "        for i in range(num_samples):\n",
    "            data, cfg = compile(ts, subkey)\n",
    "            data_list.append(data)\n",
    "            cfg_list.append(cfg)\n",
    "            key, subkey = jr.split(key)\n",
    "    elif option == \"all\":\n",
    "        from itertools import combinations\n",
    "\n",
    "        all_config = list(combinations(ts.samples(), 2))\n",
    "        for a, b in all_config:\n",
    "            data, cfg = compile(ts, subkey, a, b)\n",
    "            data_list.append(data)\n",
    "            cfg_list.append(cfg)\n",
    "    elif option == \"unphased\":\n",
    "        all_config = ts.samples().reshape(-1, 2)\n",
    "        for a, b in all_config:\n",
    "            data, cfg = compile(ts, subkey, a, b)\n",
    "            data_list.append(data)\n",
    "            cfg_list.append(cfg)\n",
    "\n",
    "    return data_list, cfg_list\n",
    "\n",
    "\n",
    "def process_data(data_list, cfg_list):\n",
    "    max_indices = jnp.array([arr.shape[0] - 1 for arr in data_list])\n",
    "    num_samples = len(max_indices)\n",
    "    lens = jnp.array([d.shape[0] for d in data_list], dtype=jnp.int32)\n",
    "    Lmax = int(lens.max())\n",
    "    Npairs = len(data_list)\n",
    "    data_pad = jnp.full((Npairs, Lmax, 2), jnp.array([1.0, 0.0]), dtype=jnp.float64)\n",
    "\n",
    "    for i, d in enumerate(data_list):\n",
    "        data_pad = data_pad.at[i, : d.shape[0], :].set(d)\n",
    "\n",
    "    deme_names = cfg_list[0].keys()\n",
    "    D = len(deme_names)\n",
    "    cfg_mat = jnp.zeros((num_samples, D), dtype=jnp.int32)\n",
    "    for i, cfg in enumerate(cfg_list):\n",
    "        for j, n in enumerate(deme_names):\n",
    "            cfg_mat = cfg_mat.at[i, j].set(cfg.get(n, 0))\n",
    "\n",
    "    unique_cfg = jnp.unique(cfg_mat, axis=0)\n",
    "\n",
    "    # Find matching indices for sampling configs\n",
    "    def find_matching_index(row, unique_arrays):\n",
    "        matches = jnp.all(row == unique_arrays, axis=1)\n",
    "        return jnp.where(matches)[0][0]\n",
    "\n",
    "    # Vectorize over all rows in `arr`\n",
    "    matching_indices = jnp.array(\n",
    "        [find_matching_index(row, unique_cfg) for row in cfg_mat]\n",
    "    )\n",
    "\n",
    "    return data_pad, deme_names, max_indices, unique_cfg, matching_indices\n",
    "\n",
    "\n",
    "def reformat_data(data_pad, matching_indices, max_indices, chunking_length):\n",
    "    unique_groups = jnp.unique(matching_indices)\n",
    "    group_unique_times = []\n",
    "    rearranged_data = []\n",
    "    new_max_indices = []\n",
    "    new_matching_indices = []\n",
    "    associated_indices = []\n",
    "    group_membership = []\n",
    "\n",
    "    # Each group is a sampling configuration\n",
    "    for group in unique_groups:\n",
    "        positions = jnp.where(\n",
    "            matching_indices == group\n",
    "        )  # extract positions matching a group\n",
    "        group_data = data_pad[\n",
    "            positions\n",
    "        ]  # find all tmrca + spans that share a sampling config\n",
    "        all_first_col = np.array(group_data[:, :, 0].flatten())\n",
    "        unique_values = np.unique(\n",
    "            all_first_col\n",
    "        )  # extra all unique tmrca associated to a sampling config\n",
    "\n",
    "        unique_value_to_index = {value: idx for idx, value in enumerate(unique_values)}\n",
    "        indices_in_mapping = np.array(\n",
    "            [unique_value_to_index[value] for value in all_first_col]\n",
    "        )\n",
    "        associated_indices.append(\n",
    "            indices_in_mapping\n",
    "        )  # figure how every tmrca in data_pad gets mapped to unique_values\n",
    "\n",
    "        group_unique_times.append(unique_values)\n",
    "        rearranged_data.append(group_data)\n",
    "        new_matching_indices.append(matching_indices[positions])\n",
    "        new_max_indices.append(max_indices[positions])\n",
    "        group_membership.append(jnp.full(indices_in_mapping.size, group))\n",
    "\n",
    "    # Find the maximum length\n",
    "    max_length = max(len(arr) for arr in group_unique_times)\n",
    "\n",
    "    # Pad each array with zeros at the end\n",
    "    padded_unique_times = []\n",
    "    for arr in group_unique_times:\n",
    "        pad_length = max_length - len(arr)\n",
    "        padded = np.pad(arr, (0, pad_length), mode=\"constant\", constant_values=0)\n",
    "        padded_unique_times.append(padded)\n",
    "\n",
    "    padded_unique_times = jnp.array(padded_unique_times)\n",
    "    rearranged_data = jnp.concatenate(rearranged_data, axis=0)\n",
    "    new_matching_indices = jnp.concatenate(new_matching_indices, axis=0)\n",
    "    new_max_indices = jnp.concatenate(new_max_indices, axis=0)\n",
    "    associated_indices = jnp.concatenate(associated_indices, axis=0)\n",
    "    group_membership = jnp.concatenate(group_membership, axis=0)\n",
    "    total_elements = group_membership.size\n",
    "    batch_size = total_elements // chunking_length\n",
    "    return (\n",
    "        padded_unique_times,\n",
    "        rearranged_data,\n",
    "        new_matching_indices,\n",
    "        new_max_indices,\n",
    "        associated_indices,\n",
    "        unique_groups,\n",
    "        batch_size,\n",
    "        group_membership,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_iicr_likelihood(\n",
    "    demo,\n",
    "    data_list,\n",
    "    cfg_list,\n",
    "    paths,\n",
    "    vec_values,\n",
    "    recombination_rate=1e-8,\n",
    "    t_min=1e-8,\n",
    "    num_t=2000,\n",
    "    k=2,\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    path_order: List[Var] = list(paths)\n",
    "    data_pad, deme_names, max_indices, unique_cfg, matching_indices = process_data(\n",
    "        data_list, cfg_list\n",
    "    )\n",
    "    # chunking_length = data_pad.shape[1]\n",
    "    # unique_times, rearranged_data, new_matching_indices, new_max_indices, associated_indices, unique_groups, batch_size, group_membership = reformat_data(data_pad, matching_indices, max_indices, chunking_length)\n",
    "\n",
    "    num_samples = len(max_indices)\n",
    "    rho = recombination_rate\n",
    "    iicr = IICRCurve(demo=demo, k=k)\n",
    "    iicr_call = iicr.curve\n",
    "\n",
    "    first_columns = data_pad[:, :, 0]\n",
    "    # Compute global max (single float value)\n",
    "    global_max = jnp.max(first_columns)\n",
    "    print(global_max)\n",
    "    t_breaks = jnp.insert(jnp.geomspace(t_min, global_max, num_t), 0, 0.0)\n",
    "\n",
    "    def loglik(\n",
    "        eta: Callable[[ScalarLike], ScalarLike],\n",
    "        r: ScalarLike,\n",
    "        data: Float[Array, \"intervals 2\"],\n",
    "        max_index: Array,\n",
    "    ) -> Scalar:\n",
    "        \"\"\"Compute the log-likelihood of the data given the demographic model.\n",
    "\n",
    "        Args:\n",
    "            eta: Coalescent rate at time t.\n",
    "            r: float, the recombination rate.\n",
    "            data: the data to compute the likelihood for. The first column is the TMRCA, and\n",
    "                  the second column is the span.\n",
    "\n",
    "        Notes:\n",
    "            - Successive spans that have the same TMRCA should be merged into one span:\n",
    "              <tmrca, span1> + <tmrca, span1> = <tmrca, span + span>.\n",
    "            - Missing data/padding indicated by span<=0.\n",
    "        \"\"\"\n",
    "        times, spans = data.T\n",
    "        i = times.argsort()\n",
    "        sorted_times = times[i]\n",
    "\n",
    "        def f(t, y, _):\n",
    "            c = jnp.where(eta(t) < 0, 1e-30, eta(t))\n",
    "            A = jnp.array([[-r, r, 0.0], [c, -2 * c, c], [0.0, 0.0, 0.0]])\n",
    "            return A.T @ y\n",
    "\n",
    "        y0 = jnp.array([1.0, 0.0, 0.0])\n",
    "        solver = dfx.Tsit5()\n",
    "        term = dfx.ODETerm(f)\n",
    "        ssc = dfx.PIDController(rtol=1e-6, atol=1e-6, jump_ts=eta.t)\n",
    "        T = times.max()\n",
    "        sol = dfx.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            0.0,\n",
    "            T,\n",
    "            dt0=0.001,\n",
    "            y0=y0,\n",
    "            stepsize_controller=ssc,\n",
    "            saveat=dfx.SaveAt(ts=sorted_times),\n",
    "        )\n",
    "\n",
    "        # invert the sorting so that cscs matches times\n",
    "        i_inv = i.argsort()\n",
    "        cscs = sol.ys[i_inv]\n",
    "\n",
    "        @vmap\n",
    "        def p(t0, csc0, t1, csc1, span):\n",
    "            p_nr_t0, p_float_t0, p_coal_t0 = csc0\n",
    "            p_nr_t1, p_float_t1, p_coal_t1 = csc1\n",
    "            # no recomb for first span - 1 positions\n",
    "            r1 = xlogy(span - 1, p_nr_t0)\n",
    "            # coalescence at t1\n",
    "            r2 = jnp.log(jnp.where(eta(t1) < 0, 1e-30, eta(t1)))\n",
    "            # back-coalescence process up to t1, depends to t0 >< t1\n",
    "            r3 = jnp.where(\n",
    "                t0 < t1,\n",
    "                jnp.log(p_float_t0) - (eta.R(t1) - eta.R(t0)),\n",
    "                jnp.log(p_float_t1),\n",
    "            )\n",
    "            return r1 + r2 + r3\n",
    "\n",
    "        ll = p(times[:-1], cscs[:-1], times[1:], cscs[1:], spans[:-1])\n",
    "        ll = jnp.dot(ll, jnp.arange(len(times[:-1])) < max_index)\n",
    "\n",
    "        # for the last position, we only know span was at least as long\n",
    "        ll += xlogy(spans[max_index], cscs[max_index, 0])\n",
    "        return ll\n",
    "\n",
    "    def get_c(params, sample_config, times):\n",
    "        ns = {name: sample_config[i] for i, name in enumerate(deme_names)}\n",
    "        single_config_func = iicr_call(params=params, num_samples=ns)\n",
    "        dictionary = jax.vmap(single_config_func, in_axes=(0))(times)\n",
    "        return dictionary[\"c\"], dictionary[\"log_s\"]\n",
    "\n",
    "    def compute_loglik(c_map, c_index, data, max_index):\n",
    "        c = c_map[c_index]\n",
    "        eta = PiecewiseConstant(c=c, t=t_breaks)\n",
    "        return loglik(eta, rho, data, max_index)\n",
    "\n",
    "    def evaluate_at_vec(vec):\n",
    "        vec_array = jnp.atleast_1d(vec)\n",
    "        params = _vec_to_dict_jax(vec_array, path_order)\n",
    "\n",
    "        c_map = []\n",
    "        log_s = []\n",
    "        for j in range(len(unique_cfg)):\n",
    "            sample_config = unique_cfg[j]\n",
    "            ns = {name: sample_config[i] for i, name in enumerate(deme_names)}\n",
    "            single_config_func = iicr_call(params=params, num_samples=ns)\n",
    "            dictionary = jax.vmap(single_config_func, in_axes=(0))(t_breaks)\n",
    "            c_map.append(dictionary[\"c\"])\n",
    "            log_s.append(dictionary[\"log_s\"])\n",
    "\n",
    "        c_map = jnp.array(c_map)\n",
    "        log_s = jnp.array(log_s)\n",
    "\n",
    "        batched_loglik = vmap(compute_loglik, in_axes=(None, 0, 0, 0))(\n",
    "            c_map, matching_indices, data_pad, max_indices\n",
    "        )\n",
    "        return -jnp.sum(batched_loglik) / num_samples  # Same as original neg_loglik\n",
    "\n",
    "    results = lax.map(evaluate_at_vec, vec_values)\n",
    "\n",
    "    # 4. Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(vec_values, results, \"r-\", linewidth=2)\n",
    "    plt.xlabel(\"vec value\")\n",
    "    plt.ylabel(\"Negative Log-Likelihood\")\n",
    "    plt.title(\"IICR Likelihood Landscape\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ad2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pulls out a SINGLE (num_samples = 1) sampling configuration that has like 55 thousand (tmrca, span)\n",
    "data_list, cfg_list = get_tmrca_data(\n",
    "    ts, key=jax.random.PRNGKey(42), num_samples=1, option=\"random\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ff230",
   "metadata": {},
   "source": [
    "Next code chunk is ONLY just plots the likelihood in which you get this big zig-zag likelihood plot. It takes 51 seconds to run on GL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c873b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jax.numpy as jnp\n",
    "# from loguru import logger\n",
    "# logger.disable(\"demestats\")\n",
    "\n",
    "# paths = {\n",
    "#     frozenset({('demes', 1, 'epochs', 0, 'end_time'),\n",
    "#             ('demes', 2, 'epochs', 0, 'end_time'),\n",
    "#             ('migrations', 0, 'end_time'),\n",
    "#             ('migrations', 1, 'end_time'),\n",
    "#             ('migrations', 2, 'start_time'),\n",
    "#             ('migrations', 3, 'start_time')}): 4000.,\n",
    "# }\n",
    "# vec_values = jnp.linspace(5, 120, 25)\n",
    "# %time result = plot_iicr_likelihood(g, data_list, cfg_list, paths, vec_values, recombination_rate=3.94 * 1e-8, num_t=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7999e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglik(\n",
    "    eta: Callable[[ScalarLike], ScalarLike],\n",
    "    r: ScalarLike,\n",
    "    data: Float[Array, \"intervals 2\"],\n",
    "    max_index: Array,\n",
    ") -> Scalar:\n",
    "    \"\"\"Compute the log-likelihood of the data given the demographic model.\n",
    "\n",
    "    Args:\n",
    "        eta: Coalescent rate at time t.\n",
    "        r: float, the recombination rate.\n",
    "        data: the data to compute the likelihood for. The first column is the TMRCA, and\n",
    "              the second column is the span.\n",
    "\n",
    "    Notes:\n",
    "        - Successive spans that have the same TMRCA should be merged into one span:\n",
    "          <tmrca, span1> + <tmrca, span1> = <tmrca, span + span>.\n",
    "        - Missing data/padding indicated by span<=0.\n",
    "    \"\"\"\n",
    "    times, spans = data.T\n",
    "    i = times.argsort()\n",
    "    sorted_times = times[i]\n",
    "\n",
    "    def f(t, y, _):\n",
    "        c = eta(t)\n",
    "        A = jnp.array([[-r, r, 0.0], [c, -2 * c, c], [0.0, 0.0, 0.0]])\n",
    "        return A.T @ y\n",
    "\n",
    "    y0 = jnp.array([1.0, 0.0, 0.0])\n",
    "    solver = dfx.Tsit5()\n",
    "    term = dfx.ODETerm(f)\n",
    "    ssc = dfx.PIDController(rtol=1e-6, atol=1e-6, jump_ts=eta.t)\n",
    "    T = times.max()\n",
    "    sol = dfx.diffeqsolve(\n",
    "        term,\n",
    "        solver,\n",
    "        0.0,\n",
    "        T,\n",
    "        dt0=0.001,\n",
    "        y0=y0,\n",
    "        stepsize_controller=ssc,\n",
    "        saveat=dfx.SaveAt(ts=sorted_times),\n",
    "    )\n",
    "\n",
    "    # invert the sorting so that cscs matches times\n",
    "    i_inv = i.argsort()\n",
    "    cscs = sol.ys[i_inv]\n",
    "\n",
    "    @vmap\n",
    "    def p(t0, csc0, t1, csc1, span):\n",
    "        p_nr_t0, p_float_t0, p_coal_t0 = csc0\n",
    "        p_nr_t1, p_float_t1, p_coal_t1 = csc1\n",
    "        # no recomb for first span - 1 positions\n",
    "        r1 = xlogy(span - 1, p_nr_t0)\n",
    "        # coalescence at t1\n",
    "        r2 = jnp.log(eta(t1))\n",
    "        # back-coalescence process up to t1, depends to t0 >< t1\n",
    "        r3 = jnp.where(\n",
    "            t0 < t1, jnp.log(p_float_t0) - (eta.R(t1) - eta.R(t0)), jnp.log(p_float_t1)\n",
    "        )\n",
    "        return r1 + r2 + r3, r1, r2, r3, eta.R(t1) - eta.R(t0)\n",
    "\n",
    "    ll, r1, r2, r3, etaR = p(times[:-1], cscs[:-1], times[1:], cscs[1:], spans[:-1])\n",
    "    ll = jnp.dot(ll, jnp.arange(len(times[:-1])) < max_index)\n",
    "\n",
    "    # for the last position, we only know span was at least as long\n",
    "    ll += xlogy(spans[max_index], cscs[max_index, 0])\n",
    "    return ll, cscs, r1, r2, r3, eta.t, etaR\n",
    "\n",
    "\n",
    "def compute_loglik(c_map, c_index, data, max_index):\n",
    "    c = c_map[c_index]\n",
    "    eta = PiecewiseConstant(c=c, t=t_breaks)\n",
    "    return loglik(eta, rho, data, max_index)\n",
    "\n",
    "\n",
    "paths = {\n",
    "    frozenset(\n",
    "        {\n",
    "            (\"demes\", 1, \"epochs\", 0, \"end_time\"),\n",
    "            (\"demes\", 2, \"epochs\", 0, \"end_time\"),\n",
    "            (\"migrations\", 0, \"end_time\"),\n",
    "            (\"migrations\", 1, \"end_time\"),\n",
    "            (\"migrations\", 2, \"start_time\"),\n",
    "            (\"migrations\", 3, \"start_time\"),\n",
    "        }\n",
    "    ): 4000.0,\n",
    "}\n",
    "path_order: List[Var] = list(paths)\n",
    "data_pad, deme_names, max_indices, unique_cfg, matching_indices = process_data(\n",
    "    data_list, cfg_list\n",
    ")\n",
    "\n",
    "num_samples = len(max_indices)\n",
    "rho = recombination_rate = 3.94 * 1e-8\n",
    "iicr = IICRCurve(demo=g, k=2)\n",
    "iicr_call = iicr.curve\n",
    "\n",
    "first_columns = data_pad[:, :, 0]\n",
    "# Compute global max (single float value)\n",
    "global_max = jnp.max(first_columns)\n",
    "print(global_max)\n",
    "t_breaks = jnp.insert(jnp.geomspace(1e-8, global_max, 30), 0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3155137",
   "metadata": {},
   "source": [
    "previous code chunk sets up everything up for the data. next code chunk stores ALL the possible computations we ever make for loglik and it takes approximately 2 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f09a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "logger.disable(\"demestats\")\n",
    "\n",
    "vec_values = jnp.linspace(5, 120, 25)\n",
    "c_list = []\n",
    "log_s_list = []\n",
    "loglik_list = []\n",
    "cscs_list = []\n",
    "r1_list = []\n",
    "r2_list = []\n",
    "r3_list = []\n",
    "etaR_list = []\n",
    "\n",
    "for vec in vec_values:\n",
    "    vec_array = jnp.atleast_1d(vec)\n",
    "    params = _vec_to_dict_jax(vec_array, path_order)\n",
    "    # print(params)\n",
    "    c_map = []\n",
    "    log_s = []\n",
    "    for j in range(len(unique_cfg)):\n",
    "        sample_config = unique_cfg[j]\n",
    "        ns = {name: sample_config[i] for i, name in enumerate(deme_names)}\n",
    "        single_config_func = iicr_call(params=params, num_samples=ns)\n",
    "        dictionary = jax.vmap(single_config_func, in_axes=(0))(t_breaks)\n",
    "        c_map.append(dictionary[\"c\"])\n",
    "        log_s.append(dictionary[\"log_s\"])\n",
    "\n",
    "    c_map = jnp.array(c_map)\n",
    "    log_s = jnp.array(log_s)\n",
    "\n",
    "    c_list.append(c_map)\n",
    "    log_s_list.append(log_s)\n",
    "\n",
    "    (\n",
    "        batched_loglik,\n",
    "        batched_cscs,\n",
    "        batched_r1,\n",
    "        batched_r2,\n",
    "        batched_r3,\n",
    "        batched_eta_jumps,\n",
    "        etaR,\n",
    "    ) = vmap(compute_loglik, in_axes=(None, 0, 0, 0))(\n",
    "        c_map, matching_indices, data_pad, max_indices\n",
    "    )\n",
    "    loglik_list.append(batched_loglik)\n",
    "    cscs_list.append(batched_cscs)\n",
    "    r1_list.append(batched_r1)\n",
    "    r2_list.append(batched_r2)\n",
    "    r3_list.append(batched_r3)\n",
    "    etaR_list.append(etaR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
