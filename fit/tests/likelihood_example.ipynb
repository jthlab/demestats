{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7adb013",
   "metadata": {},
   "source": [
    "loading all the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Mapping, Optional, Sequence, Set, Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import msprime as msp\n",
    "from scipy.optimize import LinearConstraint, minimize\n",
    "import jax.random as jr\n",
    "from jax import vmap, lax \n",
    "\n",
    "from demesinfer.coal_rate import PiecewiseConstant\n",
    "from demesinfer.constr import EventTree, constraints_for\n",
    "from demesinfer.iicr import IICRCurve\n",
    "from demesinfer.loglik.arg import loglik\n",
    "from jax.scipy.special import xlogy\n",
    "from demesinfer.sfs import ExpectedSFS\n",
    "\n",
    "import diffrax as dfx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap\n",
    "from jax.scipy.special import xlog1py, xlogy\n",
    "from jaxtyping import Array, Float, Scalar, ScalarLike\n",
    "\n",
    "from demesinfer.coal_rate import CoalRate\n",
    "\n",
    "Path = Tuple[Any, ...]\n",
    "Var = Path | Set[Path]\n",
    "Params = Mapping[Var, float]\n",
    "\n",
    "def _dict_to_vec(d: Params, keys: Sequence[Var]) -> jnp.ndarray:\n",
    "    return jnp.asarray([d[k] for k in keys], dtype=jnp.float64)\n",
    "\n",
    "def _vec_to_dict_jax(v: jnp.ndarray, keys: Sequence[Var]) -> Dict[Var, jnp.ndarray]:\n",
    "    return {k: v[i] for i, k in enumerate(keys)}\n",
    "\n",
    "def _vec_to_dict(v: jnp.ndarray, keys: Sequence[Var]) -> Dict[Var, float]:\n",
    "    return {k: float(v[i]) for i, k in enumerate(keys)}\n",
    "\n",
    "def compile(ts, subkey, a=None, b=None):\n",
    "    # using a set to pull out all unique populations that the samples can possibly belong to\n",
    "    pop_cfg = {ts.population(ts.node(n).population).metadata[\"name\"] for n in ts.samples()}\n",
    "    pop_cfg = {pop_name: 0 for pop_name in pop_cfg}\n",
    "\n",
    "    if a == None and b == None:\n",
    "        samples = jax.random.choice(subkey, ts.num_samples, shape=(2,), replace=False)\n",
    "        a, b = samples[0].item(0), samples[1].item(0)\n",
    "\n",
    "    spans = []\n",
    "    curr_t = None\n",
    "    curr_L = 0.0\n",
    "    for tree in ts.trees():\n",
    "        L = tree.interval.right - tree.interval.left\n",
    "        t = tree.tmrca(a, b)\n",
    "        if curr_t is None or t != curr_t:\n",
    "            if curr_t is not None:\n",
    "                spans.append([curr_t, curr_L])\n",
    "            curr_t = t\n",
    "            curr_L = L\n",
    "        else:\n",
    "            curr_L += L\n",
    "    spans.append([curr_t, curr_L])\n",
    "    data = jnp.asarray(spans, dtype=jnp.float64)\n",
    "    pop_cfg[ts.population(ts.node(a).population).metadata[\"name\"]] += 1\n",
    "    pop_cfg[ts.population(ts.node(b).population).metadata[\"name\"]] += 1\n",
    "    return data, pop_cfg\n",
    "\n",
    "def get_tmrca_data(ts, key=jax.random.PRNGKey(2), num_samples=200, option=\"random\"):\n",
    "    data_list = []\n",
    "    cfg_list = []\n",
    "    key, subkey = jr.split(key)\n",
    "    if option == \"random\":\n",
    "        for i in range(num_samples):\n",
    "            data, cfg = compile(ts, subkey)\n",
    "            data_list.append(data)\n",
    "            cfg_list.append(cfg)\n",
    "            key, subkey = jr.split(key)\n",
    "    elif option == \"all\":\n",
    "        from itertools import combinations\n",
    "        all_config = list(combinations(ts.samples(), 2))\n",
    "        for a, b in all_config:\n",
    "            data, cfg = compile(ts, subkey, a, b)\n",
    "            data_list.append(data)\n",
    "            cfg_list.append(cfg)\n",
    "    elif option == \"unphased\":\n",
    "        all_config = ts.samples().reshape(-1, 2)\n",
    "        for a, b in all_config:\n",
    "            data, cfg = compile(ts, subkey, a, b)\n",
    "            data_list.append(data)\n",
    "            cfg_list.append(cfg)\n",
    "\n",
    "    return data_list, cfg_list     \n",
    "\n",
    "def process_data(data_list, cfg_list):\n",
    "    max_indices = jnp.array([arr.shape[0]-1 for arr in data_list])\n",
    "    num_samples = len(max_indices)\n",
    "    lens = jnp.array([d.shape[0] for d in data_list], dtype=jnp.int32)\n",
    "    Lmax = int(lens.max())\n",
    "    Npairs = len(data_list)\n",
    "    data_pad = jnp.full((Npairs, Lmax, 2), jnp.array([1.0, 0.0]), dtype=jnp.float64)\n",
    "\n",
    "    for i, d in enumerate(data_list):\n",
    "        data_pad = data_pad.at[i, : d.shape[0], :].set(d)\n",
    "\n",
    "    deme_names = cfg_list[0].keys()\n",
    "    D = len(deme_names)\n",
    "    cfg_mat = jnp.zeros((num_samples, D), dtype=jnp.int32)\n",
    "    for i, cfg in enumerate(cfg_list):\n",
    "        for j, n in enumerate(deme_names):\n",
    "            cfg_mat = cfg_mat.at[i, j].set(cfg.get(n, 0))\n",
    "\n",
    "    unique_cfg = jnp.unique(cfg_mat, axis=0)\n",
    "\n",
    "    # Find matching indices\n",
    "    def find_matching_index(row, unique_arrays):\n",
    "        matches = jnp.all(row == unique_arrays, axis=1)\n",
    "        return jnp.where(matches)[0][0]\n",
    "\n",
    "    # Vectorize over all rows in `arr`\n",
    "    matching_indices = jnp.array([find_matching_index(row, unique_cfg) for row in cfg_mat])\n",
    "    \n",
    "    return data_pad, deme_names, max_indices, unique_cfg, matching_indices\n",
    "\n",
    "def reformat_data(data_pad, matching_indices, max_indices):\n",
    "    unique_groups = np.unique(matching_indices)\n",
    "    group_unique_times = []\n",
    "    rearranged_data = []\n",
    "    new_max_indices = []\n",
    "    new_matching_indices = []\n",
    "    associated_indices = []\n",
    "\n",
    "    for group in unique_groups:\n",
    "        group_mask = matching_indices == group\n",
    "        group_data = [arr for arr, keep in zip(data_pad, group_mask) if keep]\n",
    "        new_matching_indices.append([num for num, keep in zip(matching_indices, group_mask) if keep])\n",
    "        all_first_col = np.concatenate([arr[:, 0] for arr in group_data])\n",
    "        unique_values = np.unique(all_first_col)\n",
    "\n",
    "        unique_value_to_index = {value: idx for idx, value in enumerate(unique_values)}\n",
    "        indices_in_mapping = np.array([unique_value_to_index[value] for value in all_first_col])\n",
    "        associated_indices.append(indices_in_mapping)\n",
    "\n",
    "        group_unique_times.append(unique_values)\n",
    "        rearranged_data.append(group_data)\n",
    "        new_max_indices.append(np.array([num for num, keep in zip(max_indices, group_mask) if keep]))\n",
    "\n",
    "    # Find the maximum length\n",
    "    max_length = max(len(arr) for arr in group_unique_times)\n",
    "\n",
    "    # Pad each array with zeros at the end\n",
    "    padded_unique_times = []\n",
    "    for arr in group_unique_times:\n",
    "        pad_length = max_length - len(arr)\n",
    "        padded = np.pad(arr, (0, pad_length), mode='constant', constant_values=0)\n",
    "        padded_unique_times.append(padded)\n",
    "\n",
    "    padded_unique_times = np.array(padded_unique_times)\n",
    "    rearranged_data = jnp.array(np.vstack(rearranged_data))\n",
    "    new_matching_indices = jnp.array([x.item() for group in new_matching_indices for x in group])\n",
    "    new_max_indices = jnp.concatenate([jnp.array(arr) for arr in new_max_indices])\n",
    "    return padded_unique_times, rearranged_data, new_matching_indices, new_max_indices, associated_indices, unique_groups\n",
    "\n",
    "def loglik_ode(eta: CoalRate, r: ScalarLike, times: Float[Array, \"intervals 1\"]) -> Scalar:\n",
    "    \"\"\"Compute the log-likelihood of the data given the demographic model.\n",
    "\n",
    "    Args:\n",
    "        eta: Coalescent rate at time t.\n",
    "        r: float, the recombination rate.\n",
    "        data: the data to compute the likelihood for. The first column is the TMRCA, and\n",
    "              the second column is the span.\n",
    "\n",
    "    Notes:\n",
    "        - Successive spans that have the same TMRCA should be merged into one span:\n",
    "          <tmrca, span1> + <tmrca, span1> = <tmrca, span + span>.\n",
    "        - Missing data/padding indicated by span<=0.\n",
    "    \"\"\"\n",
    "    # times = data.T\n",
    "    i = times.argsort()\n",
    "    sorted_times = times[i]\n",
    "\n",
    "    def f(t, y, _):\n",
    "        c = eta(t)\n",
    "        A = jnp.array([[-r, r, 0.0], [c, -2 * c, c], [0.0, 0.0, 0.0]])\n",
    "        return A.T @ y\n",
    "\n",
    "    y0 = jnp.array([1.0, 0.0, 0.0])\n",
    "    solver = dfx.Tsit5()\n",
    "    term = dfx.ODETerm(f)\n",
    "    ssc = dfx.PIDController(rtol=1e-6, atol=1e-6, jump_ts=eta.jumps)\n",
    "    T = times.max()\n",
    "    sol = dfx.diffeqsolve(\n",
    "        term,\n",
    "        solver,\n",
    "        0.0,\n",
    "        T,\n",
    "        dt0=0.001,\n",
    "        y0=y0,\n",
    "        stepsize_controller=ssc,\n",
    "        saveat=dfx.SaveAt(ts=sorted_times),\n",
    "    )\n",
    "\n",
    "    # invert the sorting so that cscs matches times\n",
    "    i_inv = i.argsort()\n",
    "    cscs = sol.ys[i_inv]\n",
    "    return cscs\n",
    "\n",
    "def plot_iicr_likelihood(demo, data_list, cfg_list, paths, vec_values, recombination_rate=1e-8, t_min=1e-8, num_t=2000, k=2):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    path_order: List[Var] = list(paths)\n",
    "    data_pad, deme_names, max_indices, unique_cfg, matching_indices = process_data(data_list, cfg_list)\n",
    "    unique_times, data_pad, matching_indices, max_indices, associated_indices, unique_groups = reformat_data(data_pad, matching_indices, max_indices)\n",
    "\n",
    "    num_samples = len(max_indices)\n",
    "    first_columns = data_pad[:, :, 0]\n",
    "    # Compute global max (single float value)\n",
    "    global_max = jnp.max(first_columns)\n",
    "    print(global_max)\n",
    "    t_breaks = jnp.insert(jnp.geomspace(t_min, global_max, num_t), 0, 0.0)\n",
    "    rho = recombination_rate\n",
    "    iicr = IICRCurve(demo=demo, k=k)\n",
    "    iicr_call = jax.jit(iicr.__call__)\n",
    "    chunking_length = data_pad.shape[1]\n",
    "\n",
    "    def compute_ode(c, times):\n",
    "        eta = PiecewiseConstant(c=c, t=t_breaks)\n",
    "        return loglik_ode(eta, rho, times)\n",
    "    \n",
    "    def compute_loglik(data, cscs, max_index, c_map, c_index):\n",
    "        c = c_map[c_index]\n",
    "        eta = PiecewiseConstant(c=c, t=t_breaks)\n",
    "        times, spans = data.T\n",
    "        @vmap\n",
    "        def p(t0, csc0, t1, csc1, span):\n",
    "            p_nr_t0, p_float_t0, p_coal_t0 = csc0\n",
    "            p_nr_t1, p_float_t1, p_coal_t1 = csc1\n",
    "            # no recomb for first span - 1 positions\n",
    "            r1 = xlogy(span - 1, p_nr_t0)\n",
    "            # coalescence at t1\n",
    "            r2 = jnp.log(eta(t1))\n",
    "            # back-coalescence process up to t1, depends to t0 >< t1\n",
    "            r3 = jnp.where(\n",
    "                t0 < t1, jnp.log(p_float_t0) - eta.R(t0, t1), jnp.log(p_float_t1)\n",
    "            )\n",
    "            return r1 + r2 + r3\n",
    "\n",
    "        ll = p(times[:-1], cscs[:-1], times[1:], cscs[1:], spans[:-1])\n",
    "        ll = jnp.dot(ll, jnp.arange(len(times[:-1])) < max_index)\n",
    "        \n",
    "        # for the last position, we only know span was at least as long\n",
    "        ll += xlogy(spans[max_index], cscs[max_index, 0])\n",
    "        return ll\n",
    "    \n",
    "    def evaluate_at_vec(vec):\n",
    "        vec_array = jnp.atleast_1d(vec)\n",
    "        params = _vec_to_dict_jax(vec_array, path_order)\n",
    "\n",
    "        c_map = jax.vmap(lambda cfg: iicr_call(params=params, t=t_breaks, num_samples=dict(zip(deme_names, cfg)))[\"c\"])(\n",
    "            jnp.array(unique_cfg)\n",
    "        )\n",
    "        \n",
    "        all_cscs = jax.vmap(compute_ode, in_axes=(0, 0))(c_map, unique_times)\n",
    "\n",
    "        final_cscs = []\n",
    "        for i in unique_groups:\n",
    "            final_cscs.append(all_cscs[i][associated_indices[i]].reshape(int(associated_indices[i].shape[0]/chunking_length), chunking_length, 3))\n",
    "\n",
    "        # Batched over cfg_mat and all_tmrca_spans \n",
    "        batched_loglik = vmap(compute_loglik, in_axes=(0, 0, 0, None, 0))(data_pad, jnp.vstack(final_cscs), max_indices, c_map, matching_indices)\n",
    "        return -jnp.sum(batched_loglik) / num_samples  # Same as original neg_loglik\n",
    "\n",
    "    # Outer vmap: Parallelize across vec_values\n",
    "    # batched_neg_loglik = vmap(evaluate_at_vec)  # in_axes=0 is default\n",
    "\n",
    "    # 3. Compute all values (runs on GPU/TPU if available)\n",
    "    # results = batched_neg_loglik(vec_values) \n",
    "    results = lax.map(evaluate_at_vec, vec_values)\n",
    "\n",
    "    # 4. Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(vec_values, results, 'r-', linewidth=2)\n",
    "    plt.xlabel(\"vec value\")\n",
    "    plt.ylabel(\"Negative Log-Likelihood\")\n",
    "    plt.title(\"IICR Likelihood Landscape\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_sfs_likelihood(demo, paths, vec_values, afs, afs_samples, theta=None, sequence_length=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    path_order: List[Var] = list(paths)\n",
    "    esfs = ExpectedSFS(demo, num_samples=afs_samples)\n",
    "\n",
    "    def sfs_loglik(afs, esfs, sequence_length, theta):\n",
    "        afs = afs.flatten()[1:-1]\n",
    "        esfs = esfs.flatten()[1:-1]\n",
    "        \n",
    "        if theta:\n",
    "            assert(sequence_length)\n",
    "            tmp = esfs * sequence_length * theta\n",
    "            return jnp.sum(-tmp + xlogy(afs, tmp))\n",
    "        else:\n",
    "            return jnp.sum(xlogy(afs, esfs/esfs.sum()))\n",
    "    \n",
    "    def evaluate_at_vec(vec):\n",
    "        vec_array = jnp.atleast_1d(vec)\n",
    "        params = _vec_to_dict_jax(vec_array, path_order)\n",
    "        e1 = esfs(params)\n",
    "        return -sfs_loglik(afs, e1, sequence_length, theta)\n",
    "\n",
    "    # Outer vmap: Parallelize across vec_values\n",
    "    # batched_neg_loglik = vmap(evaluate_at_vec)  # in_axes=0 is default\n",
    "\n",
    "    # 3. Compute all values (runs on GPU/TPU if available)\n",
    "    # results = batched_neg_loglik(vec_values) \n",
    "    results = lax.map(evaluate_at_vec, vec_values)\n",
    "\n",
    "    # 4. Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(vec_values, results, 'r-', linewidth=2)\n",
    "    plt.xlabel(\"vec value\")\n",
    "    plt.ylabel(\"Negative Log-Likelihood\")\n",
    "    plt.title(\"SFS Likelihood Landscape\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_likelihood(demo, data_list, cfg_list, paths, vec_values, afs, afs_samples, theta=None, sequence_length=None, recombination_rate=1e-8, t_min=1e-8, num_t=2000, k=2):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    path_order: List[Var] = list(paths)\n",
    "    data_pad, cfg_mat, deme_names, max_indices, unique_cfg, matching_indices = process_data(data_list, cfg_list)\n",
    "    num_samples = len(max_indices)\n",
    "    first_columns = data_pad[:, :, 0]\n",
    "    # Compute global max (single float value)\n",
    "    global_max = jnp.max(first_columns)\n",
    "    print(global_max)\n",
    "    t_breaks = jnp.insert(jnp.geomspace(t_min, global_max, num_t), 0, 0.0)\n",
    "    rho = recombination_rate\n",
    "    iicr = IICRCurve(demo=demo, k=k)\n",
    "    iicr_call = jax.jit(iicr.__call__)\n",
    "    esfs = ExpectedSFS(demo, num_samples=afs_samples)\n",
    "\n",
    "    def compute_loglik(c_map, c_index, data, max_index):\n",
    "        c = c_map[c_index]\n",
    "        eta = PiecewiseConstant(c=c, t=t_breaks)\n",
    "        return loglik(eta, rho, data, max_index)\n",
    "    \n",
    "    def sfs_loglik(afs, esfs, sequence_length, theta):\n",
    "        afs = afs.flatten()[1:-1]\n",
    "        esfs = esfs.flatten()[1:-1]\n",
    "        \n",
    "        if theta:\n",
    "            assert(sequence_length)\n",
    "            tmp = esfs * sequence_length * theta\n",
    "            return jnp.sum(-tmp + xlogy(afs, tmp))\n",
    "        else:\n",
    "            return jnp.sum(xlogy(afs, esfs/esfs.sum()))\n",
    "    \n",
    "    def evaluate_at_vec(vec):\n",
    "        vec_array = jnp.atleast_1d(vec)\n",
    "        params = _vec_to_dict_jax(vec_array, path_order)\n",
    "\n",
    "        c_map = jax.vmap(lambda cfg: iicr_call(params=params, t=t_breaks, num_samples=dict(zip(deme_names, cfg)))[\"c\"])(\n",
    "            jnp.array(unique_cfg)\n",
    "        )\n",
    "        \n",
    "        # Batched over cfg_mat and all_tmrca_spans \n",
    "        batched_loglik = vmap(compute_loglik, in_axes=(None, 0, 0, 0))(c_map, matching_indices, data_pad, max_indices)\n",
    "\n",
    "        e1 = esfs(params)\n",
    "        return (-jnp.sum(batched_loglik) / num_samples) + -sfs_loglik(afs, e1, sequence_length, theta) # Same as original neg_loglik\n",
    "\n",
    "    # Outer vmap: Parallelize across vec_values\n",
    "    # batched_neg_loglik = vmap(evaluate_at_vec)  # in_axes=0 is default\n",
    "\n",
    "    # 3. Compute all values (runs on GPU/TPU if available)\n",
    "    # results = batched_neg_loglik(vec_values) \n",
    "    results = lax.map(evaluate_at_vec, vec_values)\n",
    "\n",
    "    # 4. Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(vec_values, results, 'r-', linewidth=2)\n",
    "    plt.xlabel(\"vec value\")\n",
    "    plt.ylabel(\"Negative Log-Likelihood\")\n",
    "    plt.title(\"SFS and IICR Likelihood Landscape\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "def fit(\n",
    "    demo,\n",
    "    data_list, \n",
    "    cfg_list,\n",
    "    paths: Params,\n",
    "    afs,\n",
    "    afs_samples,\n",
    "    *,\n",
    "    k: int = 2,\n",
    "    t_min: float = 1e-8,\n",
    "    # t_max: float,\n",
    "    num_t: int = 2000,\n",
    "    method: str = \"trust-constr\",\n",
    "    options: Optional[dict] = None,\n",
    "    recombination_rate: float = 1e-8,\n",
    "    sequence_length: float = None,\n",
    "    theta: float = None,\n",
    "):\n",
    "    data_pad, cfg_mat, deme_names, max_indices, unique_cfg, matching_indices = process_data(data_list, cfg_list)\n",
    "    num_samples = len(max_indices)\n",
    "\n",
    "    path_order: List[Var] = list(paths)\n",
    "    x0 = _dict_to_vec(paths, path_order)\n",
    "    et = EventTree(demo)\n",
    "\n",
    "    cons = constraints_for(et, *path_order)\n",
    "    linear_constraints: list[LinearConstraint] = []\n",
    "\n",
    "    Aeq, beq = cons[\"eq\"]\n",
    "    if Aeq.size:\n",
    "        linear_constraints.append(LinearConstraint(Aeq, beq, beq))\n",
    "\n",
    "    G, h = cons[\"ineq\"]\n",
    "    if G.size:\n",
    "        lower = -jnp.inf * jnp.ones_like(h)\n",
    "        linear_constraints.append(LinearConstraint(G, lower, h))\n",
    "\n",
    "    first_columns = data_pad[:, :, 0]\n",
    "    # Compute global max (single float value)\n",
    "    global_max = jnp.max(first_columns)\n",
    "    t_breaks = jnp.insert(jnp.geomspace(t_min, global_max, num_t), 0, 0.0)\n",
    "    rho = recombination_rate\n",
    "    iicr = IICRCurve(demo=demo, k=k)\n",
    "    iicr_call = jax.jit(iicr.__call__)\n",
    "    esfs = ExpectedSFS(demo, num_samples=afs_samples)\n",
    "\n",
    "    def compute_loglik(c_map, c_index, data, max_index):\n",
    "        c = c_map[c_index]\n",
    "        eta = PiecewiseConstant(c=c, t=t_breaks)\n",
    "        return loglik(eta, rho, data, max_index)\n",
    "    \n",
    "    def sfs_loglik(afs, esfs, sequence_length, theta):\n",
    "        afs = afs.flatten()[1:-1]\n",
    "        esfs = esfs.flatten()[1:-1]\n",
    "        \n",
    "        if theta:\n",
    "            assert(sequence_length)\n",
    "            tmp = esfs * sequence_length * theta\n",
    "            return jnp.sum(-tmp + xlogy(afs, tmp))\n",
    "        else:\n",
    "            return jnp.sum(xlogy(afs, esfs/esfs.sum()))\n",
    "    \n",
    "    @jax.value_and_grad\n",
    "    def neg_loglik(vec):\n",
    "        params = _vec_to_dict_jax(vec, path_order)\n",
    "        c_map = jax.vmap(lambda cfg: iicr_call(params=params, t=t_breaks, num_samples=dict(zip(deme_names, cfg)))[\"c\"])(\n",
    "            jnp.array(unique_cfg)\n",
    "        )\n",
    "        \n",
    "        # Batched over cfg_mat and all_tmrca_spans \n",
    "        batched_loglik = vmap(compute_loglik, in_axes=(None, 0, 0, 0))(c_map, matching_indices, data_pad, max_indices)\n",
    "        \n",
    "        likelihood = jnp.sum(batched_loglik)\n",
    "        e1 = esfs(params)\n",
    "\n",
    "        return (-likelihood / num_samples) + -sfs_loglik(afs, e1, sequence_length, theta)\n",
    "\n",
    "    res = minimize(\n",
    "        fun=lambda x: float(neg_loglik(x)[0]),\n",
    "        # fun=lambda x: float(neg_loglik(x)),\n",
    "        x0=jnp.asarray(x0),\n",
    "        jac=lambda x: jnp.asarray(neg_loglik(x)[1], dtype=float),\n",
    "        method=method,\n",
    "        # bounds = [(3000. / 5000., 7000. / 5000.)],\n",
    "        constraints=linear_constraints,\n",
    "    )\n",
    "\n",
    "    return _vec_to_dict(jnp.asarray(res.x), path_order)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacde1c0",
   "metadata": {},
   "source": [
    "Running the snake model simulation: Please note that this simulation takes like 6-7 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbed758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msprime as msp\n",
    "import demes\n",
    "import demesdraw\n",
    "import numpy as np\n",
    "\n",
    "# Create demography object\n",
    "demo = msp.Demography()\n",
    "\n",
    "# Add populations\n",
    "demo.add_population(initial_size=4000, name=\"anc\")\n",
    "demo.add_population(initial_size=500, name=\"P0\", growth_rate=-np.log(3000 / 500)/66)\n",
    "demo.add_population(initial_size=500, name=\"P1\", growth_rate=-np.log(3000 / 500)/66)\n",
    "demo.add_population(initial_size=100, name=\"P2\", growth_rate=-np.log(3000 / 100)/66)\n",
    "demo.add_population(initial_size=800, name=\"P3\", growth_rate=-np.log(3000 / 800)/66)\n",
    "demo.add_population(initial_size=500, name=\"P4\", growth_rate=-np.log(3000 / 500)/66)\n",
    "\n",
    "# Set initial migration rate\n",
    "demo.set_symmetric_migration_rate(populations=(\"P0\", \"P1\"), rate=0.0001)\n",
    "demo.set_symmetric_migration_rate(populations=(\"P1\", \"P2\"), rate=0.0001)\n",
    "demo.set_symmetric_migration_rate(populations=(\"P2\", \"P3\"), rate=0.0001)\n",
    "demo.set_symmetric_migration_rate(populations=(\"P3\", \"P4\"), rate=0.0001)\n",
    "\n",
    "\n",
    "# population growth at 500 generations\n",
    "demo.add_population_parameters_change(\n",
    "    time=66,\n",
    "    initial_size=3000,  # Bottleneck: reduce to 1000 individuals\n",
    "    population=\"P0\",\n",
    "    growth_rate=0\n",
    ")\n",
    "demo.add_population_parameters_change(\n",
    "    time=66,\n",
    "    initial_size=3000,  # Bottleneck: reduce to 1000 individuals\n",
    "    population=\"P1\",\n",
    "    growth_rate=0\n",
    ")\n",
    "demo.add_population_parameters_change(\n",
    "    time=66,\n",
    "    initial_size=3000,  # Bottleneck: reduce to 1000 individuals\n",
    "    population=\"P2\",\n",
    "    growth_rate=0\n",
    ")\n",
    "demo.add_population_parameters_change(\n",
    "    time=66,\n",
    "    initial_size=3000,  # Bottleneck: reduce to 1000 individuals\n",
    "    population=\"P3\",\n",
    "    growth_rate=0\n",
    ")\n",
    "demo.add_population_parameters_change(\n",
    "    time=66,\n",
    "    initial_size=3000,  # Bottleneck: reduce to 1000 individuals\n",
    "    population=\"P4\",\n",
    "    growth_rate=0\n",
    ")\n",
    "\n",
    "# Migration rate change changed to 0.001 AFTER 500 generation (going into the past)\n",
    "demo.add_migration_rate_change(\n",
    "    time=66,\n",
    "    rate=0.0005, \n",
    "    source=\"P0\",\n",
    "    dest=\"P1\"\n",
    ")\n",
    "demo.add_migration_rate_change(\n",
    "    time=66,\n",
    "    rate=0.0005, \n",
    "    source=\"P1\",\n",
    "    dest=\"P0\"\n",
    ")\n",
    "demo.add_migration_rate_change(\n",
    "    time=66,\n",
    "    rate=0.0005, \n",
    "    source=\"P1\",\n",
    "    dest=\"P2\"\n",
    ")\n",
    "demo.add_migration_rate_change(\n",
    "    time=66,\n",
    "    rate=0.0005, \n",
    "    source=\"P2\",\n",
    "    dest=\"P1\"\n",
    ")\n",
    "demo.add_migration_rate_change(\n",
    "    time=66,\n",
    "    rate=0.0005, \n",
    "    source=\"P2\",\n",
    "    dest=\"P3\"\n",
    ")\n",
    "demo.add_migration_rate_change(\n",
    "    time=66,\n",
    "    rate=0.0005, \n",
    "    source=\"P3\",\n",
    "    dest=\"P2\"\n",
    ")\n",
    "demo.add_migration_rate_change(\n",
    "    time=66,\n",
    "    rate=0.0005, \n",
    "    source=\"P3\",\n",
    "    dest=\"P4\"\n",
    ")\n",
    "demo.add_migration_rate_change(\n",
    "    time=66,\n",
    "    rate=0.0005, \n",
    "    source=\"P4\",\n",
    "    dest=\"P3\"\n",
    ")\n",
    "\n",
    "# THEN add the older events (population split at 1000)\n",
    "demo.add_population_split(time=5000, derived=[\"P0\", \"P1\", \"P2\", \"P3\", \"P4\"], ancestral=\"anc\")\n",
    "\n",
    "# Visualize the demography\n",
    "g = demo.to_demes()\n",
    "demesdraw.tubes(g, log_time=True)\n",
    "\n",
    "sample_size = 10\n",
    "samples = {f\"P{i}\": sample_size for i in range(5)}\n",
    "anc = msp.sim_ancestry(samples=samples, demography=demo, recombination_rate=3.94 * 1e-8, sequence_length=1e8, random_seed = 12)\n",
    "ts = msp.sim_mutations(anc, rate=2.54 * 1e-8, random_seed = 12)\n",
    "\n",
    "afs_samples = {f\"P{i}\": sample_size*2 for i in range(5)}\n",
    "afs = ts.allele_frequency_spectrum(sample_sets=[ts.samples([1]), ts.samples([2]), ts.samples([3]), ts.samples([4]), ts.samples([5])], span_normalise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa186d",
   "metadata": {},
   "source": [
    "pulling out 400 samples. This takes 10min 40s, if you don't want to wait that long just decrease num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time data_list, cfg_list = get_tmrca_data(ts, key=jax.random.PRNGKey(42), num_samples=400, option=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991235f9",
   "metadata": {},
   "source": [
    "The next piece is just running everything so I can play around with the likelihood computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a784ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    ('migrations', 0, 'rate'): 4000.,\n",
    "}\n",
    "\n",
    "path_order: List[Var] = list(paths)\n",
    "data_pad, deme_names, max_indices, unique_cfg, matching_indices = process_data(data_list, cfg_list)\n",
    "unique_times, data_pad, matching_indices, max_indices, associated_indices, unique_groups = reformat_data(data_pad, matching_indices, max_indices)\n",
    "\n",
    "num_samples = len(max_indices)\n",
    "first_columns = data_pad[:, :, 0]\n",
    "# Compute global max (single float value)\n",
    "global_max = jnp.max(first_columns)\n",
    "print(global_max)\n",
    "t_breaks = jnp.insert(jnp.geomspace(1e-8, global_max, 2000), 0, 0.0)\n",
    "rho = 1e-8\n",
    "iicr = IICRCurve(demo=g, k=2)\n",
    "iicr_call = jax.jit(iicr.__call__)\n",
    "chunking_length = data_pad.shape[1]\n",
    "\n",
    "vec = [0.1]\n",
    "vec_array = jnp.atleast_1d(vec)\n",
    "params = _vec_to_dict_jax(vec_array, path_order)\n",
    "\n",
    "# def compute_ode(c, times):\n",
    "#     eta = PiecewiseConstant(c=c, t=t_breaks)\n",
    "#     return loglik_ode(eta, rho, times)\n",
    "\n",
    "def compute_loglik(data, cscs, max_index, c_map, c_index):\n",
    "    c = c_map[c_index]\n",
    "    eta = PiecewiseConstant(c=c, t=t_breaks)\n",
    "    times, spans = data.T\n",
    "    @vmap\n",
    "    def p(t0, csc0, t1, csc1, span):\n",
    "        p_nr_t0, p_float_t0, p_coal_t0 = csc0\n",
    "        p_nr_t1, p_float_t1, p_coal_t1 = csc1\n",
    "        # no recomb for first span - 1 positions\n",
    "        r1 = xlogy(span - 1, p_nr_t0)\n",
    "        # coalescence at t1\n",
    "        r2 = jnp.log(eta(t1))\n",
    "        # back-coalescence process up to t1, depends to t0 >< t1\n",
    "        r3 = jnp.where(\n",
    "            t0 < t1, jnp.log(p_float_t0) - eta.R(t0, t1), jnp.log(p_float_t1)\n",
    "        )\n",
    "        return r1 + r2 + r3\n",
    "\n",
    "    ll = p(times[:-1], cscs[:-1], times[1:], cscs[1:], spans[:-1])\n",
    "    ll = jnp.dot(ll, jnp.arange(len(times[:-1])) < max_index)\n",
    "\n",
    "    # for the last position, we only know span was at least as long\n",
    "    ll += xlogy(spans[max_index], cscs[max_index, 0])\n",
    "    return ll\n",
    "\n",
    "def evaluate_at_vec(vec):\n",
    "    vec_array = jnp.atleast_1d(vec)\n",
    "    params = _vec_to_dict_jax(vec_array, path_order)\n",
    "    def compute_c(sample_config, times):\n",
    "        # Convert sample_config (array) to dictionary of population sizes\n",
    "        ns = {name: sample_config[i] for i, name in enumerate(deme_names)}\n",
    "\n",
    "        # Compute IICR and log-likelihood\n",
    "        c = iicr_call(params=params, t=t_breaks, num_samples=ns)[\"c\"]\n",
    "        eta = PiecewiseConstant(c=c, t=t_breaks)\n",
    "        cscs = loglik_ode(eta, rho, times)\n",
    "        return c, cscs\n",
    "    c_map, all_cscs = vmap(compute_c, in_axes=(0, 0))(unique_cfg, unique_times)\n",
    "\n",
    "\n",
    "#     c_map = jax.vmap(lambda cfg: iicr_call(params=params, t=t_breaks, num_samples=dict(zip(deme_names, cfg)))[\"c\"])(\n",
    "#         jnp.array(unique_cfg)\n",
    "#     )\n",
    "\n",
    "#    all_cscs = jax.vmap(compute_ode, in_axes=(0, 0))(c_map, unique_times)\n",
    "\n",
    "    final_cscs = []\n",
    "    for i in unique_groups:\n",
    "        final_cscs.append(all_cscs[i][associated_indices[i]].reshape(int(associated_indices[i].shape[0]/chunking_length), chunking_length, 3))\n",
    "\n",
    "    # Batched over cfg_mat and all_tmrca_spans \n",
    "    batched_loglik = vmap(compute_loglik, in_axes=(0, 0, 0, None, 0))(data_pad, jnp.vstack(final_cscs), max_indices, c_map, matching_indices)\n",
    "    return -jnp.sum(batched_loglik) / num_samples  # Same as original neg_loglik\n",
    "\n",
    "vec_values = jnp.linspace(0.0001, 0.0010, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b421a6c",
   "metadata": {},
   "source": [
    "Timing how long it takes to run one likelihood evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time evaluate_at_vec([0.0005])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e78fb21",
   "metadata": {},
   "source": [
    "Timing how long it takes to run 10 likelihood evaluations with lax.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d756e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time results = lax.map(evaluate_at_vec, vec_values)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
